# Basic configuration for diffusion model training
defaults:
  - _self_
  - network: advanced_mlp # Options: ddpmpp, ddpm, edm, advanced_mlp, mlp
  - dataset: swiss_roll # Options: swiss_roll, cifar10
  - precond: edm  # This will load config/precond/edm.yaml
  - loss: edm    # This will load config/loss/edm.yaml
  - model: edm

# Wandb logger configuration
use_logger: True
project_name: "edm-2d"


# Training configuration
train:
  # Post-training configuration
  post_training: False  # Whether to use post-training mode
  pretrained_path: null  # Path or URL to pretrained model
  pretrained_cache_dir: null  # Optional custom cache directory for pretrained models
  
  # Standard training configuration
  batch_size: 256
  num_workers: 4
  learning_rate: 0.001
  max_epochs: 500
  accelerator: "auto"
  devices: 1
  precision: "16-mixed"
  amp_backend: "native"
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  sampling_interval: 5  # Generate samples every 50 epochs
   
  # Entropy configuration
  entropy_interval: 5
  entropy_num_timesteps: 100
  entropic_sampler: False  # Flag to enable entropy-based sampling
  entropic_warmup_epochs: 2    # Number of epochs to train with standard sampling before switching
  entropic_num_grid_points: 100  # Number of points in entropy grid


# Sampling configuration
sampling: 
  batch_size: 500
  n_steps: 10
  # Evaluation samples
  eval_samples: 1000
  eval_batch_size: 100

# Logging configuration
logging:
  save_dir: "results"
  name: "diffusion_experiment"
  version: "1"
  log_every_n_steps: 50
  save_top_k: 3
  monitor: "val_loss"
  mode: "min"

# Visualization configuration
viz:
  plot_every_n_epochs: 5
  num_samples: 1000
  figsize: [6, 6]
  save_dir: "plots"
