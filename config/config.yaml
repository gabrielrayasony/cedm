# Basic configuration for diffusion model training
defaults:
  - network: mlp
  - dataset: swiss_roll
  - precond: edm  # This will load config/precond/edm.yaml
  - loss: edm    # This will load config/loss/edm.yaml
  - model: edm

# Training configuration
train:
  batch_size: 256
  num_workers: 4
  learning_rate: 0.001
  max_epochs: 500
  accelerator: "auto"
  devices: 1
  precision: "16-mixed"
  amp_backend: "native"
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  sampling_interval: 5  # Generate samples every 50 epochs
  use_entropic_sampler: True  # Flag to enable entropy-based sampling
  entropic_warmup_epochs: 2    # Number of epochs to train with standard sampling before switching
  entropic_num_grid_points: 100  # Number of points in entropy grid

  # Entropy configuration
  entropy_interval: 5
  entropy_num_timesteps: 100

# Sampling configuration
sampling:
  batch_size: 500
  n_steps: 10
  # Evaluation samples
  eval_samples: 1000
  eval_batch_size: 100

# Logging configuration
logging:
  save_dir: "results"
  name: "diffusion_experiment"
  version: "1"
  log_every_n_steps: 50
  save_top_k: 3
  monitor: "val_loss"
  mode: "min"

# Visualization configuration
viz:
  plot_every_n_epochs: 5
  num_samples: 1000
  figsize: [6, 6]
  save_dir: "plots"
